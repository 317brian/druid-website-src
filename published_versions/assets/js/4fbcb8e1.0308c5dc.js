"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[9992],{28453:(e,o,t)=>{t.d(o,{R:()=>n,x:()=>i});var a=t(96540);const d={},r=a.createContext(d);function n(e){const o=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function i(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:n(e.components),a.createElement(r.Provider,{value:o},e.children)}},36698:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>s,contentTitle:()=>i,default:()=>h,frontMatter:()=>n,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"tutorials/tutorial-batch-hadoop","title":"Load batch data using Apache Hadoop","description":"\x3c!--","source":"@site/docs/latest/tutorials/tutorial-batch-hadoop.md","sourceDirName":"tutorials","slug":"/tutorials/tutorial-batch-hadoop","permalink":"/docs/latest/tutorials/tutorial-batch-hadoop","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"tutorial-batch-hadoop","title":"Load batch data using Apache Hadoop","sidebar_label":"Load from Apache Hadoop"},"sidebar":"docs","previous":{"title":"JDBC connector tutorial","permalink":"/docs/latest/tutorials/tutorial-jdbc"},"next":{"title":"Kerberized HDFS deep storage","permalink":"/docs/latest/tutorials/tutorial-kerberos-hadoop"}}');var d=t(74848),r=t(28453);const n={id:"tutorial-batch-hadoop",title:"Load batch data using Apache Hadoop",sidebar_label:"Load from Apache Hadoop"},i=void 0,s={},l=[{value:"Install Docker",id:"install-docker",level:2},{value:"Build the Hadoop docker image",id:"build-the-hadoop-docker-image",level:2},{value:"Setup the Hadoop docker cluster",id:"setup-the-hadoop-docker-cluster",level:2},{value:"Create temporary shared directory",id:"create-temporary-shared-directory",level:3},{value:"Configure /etc/hosts",id:"configure-etchosts",level:3},{value:"Start the Hadoop container",id:"start-the-hadoop-container",level:3},{value:"Accessing the Hadoop container shell",id:"accessing-the-hadoop-container-shell",level:4},{value:"Copy input data to the Hadoop container",id:"copy-input-data-to-the-hadoop-container",level:3},{value:"Setup HDFS directories",id:"setup-hdfs-directories",level:3},{value:"Configure Druid to use Hadoop",id:"configure-druid-to-use-hadoop",level:2},{value:"Copy Hadoop configuration to Druid classpath",id:"copy-hadoop-configuration-to-druid-classpath",level:3},{value:"Update Druid segment and log storage",id:"update-druid-segment-and-log-storage",level:3},{value:"Disable local deep storage and enable HDFS deep storage",id:"disable-local-deep-storage-and-enable-hdfs-deep-storage",level:4},{value:"Disable local log storage and enable HDFS log storage",id:"disable-local-log-storage-and-enable-hdfs-log-storage",level:4},{value:"Restart Druid cluster",id:"restart-druid-cluster",level:3},{value:"Load batch data",id:"load-batch-data",level:2},{value:"Querying your data",id:"querying-your-data",level:2},{value:"Cleanup",id:"cleanup",level:2},{value:"Further reading",id:"further-reading",level:2}];function c(e){const o={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(o.p,{children:"This tutorial shows you how to load data files into Apache Druid using a remote Hadoop cluster."}),"\n",(0,d.jsxs)(o.p,{children:["For this tutorial, we'll assume that you've already completed the previous\n",(0,d.jsx)(o.a,{href:"/docs/latest/tutorials/tutorial-batch",children:"batch ingestion tutorial"})," using Druid's native batch ingestion system and are using the\nautomatic single-machine configuration as described in the ",(0,d.jsx)(o.a,{href:"/docs/latest/operations/single-server",children:"quickstart"}),"."]}),"\n",(0,d.jsx)(o.h2,{id:"install-docker",children:"Install Docker"}),"\n",(0,d.jsxs)(o.p,{children:["This tutorial requires ",(0,d.jsx)(o.a,{href:"https://docs.docker.com/install/",children:"Docker"})," to be installed on the tutorial machine."]}),"\n",(0,d.jsx)(o.p,{children:"Once the Docker install is complete, please proceed to the next steps in the tutorial."}),"\n",(0,d.jsx)(o.h2,{id:"build-the-hadoop-docker-image",children:"Build the Hadoop docker image"}),"\n",(0,d.jsx)(o.p,{children:"For this tutorial, we've provided a Dockerfile for a Hadoop 3.3.6 cluster, which we'll use to run the batch indexing task."}),"\n",(0,d.jsxs)(o.p,{children:["This Dockerfile and related files are located at ",(0,d.jsx)(o.code,{children:"quickstart/tutorial/hadoop/docker"}),"."]}),"\n",(0,d.jsxs)(o.p,{children:["From the ",(0,d.jsx)(o.code,{children:"apache-druid-32.0.0"}),' package root, run the following commands to build a Docker image named "druid-hadoop-demo" with version tag "3.3.6":']}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"cd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:3.3.6 .\n"})}),"\n",(0,d.jsxs)(o.p,{children:["This will start building the Hadoop image. Once the image build is done, you should see the message ",(0,d.jsx)(o.code,{children:"Successfully tagged druid-hadoop-demo:3.3.6"})," printed to the console."]}),"\n",(0,d.jsx)(o.h2,{id:"setup-the-hadoop-docker-cluster",children:"Setup the Hadoop docker cluster"}),"\n",(0,d.jsx)(o.h3,{id:"create-temporary-shared-directory",children:"Create temporary shared directory"}),"\n",(0,d.jsx)(o.p,{children:"We'll need a shared folder between the host and the Hadoop container for transferring some files."}),"\n",(0,d.jsxs)(o.p,{children:["Let's create some folders under ",(0,d.jsx)(o.code,{children:"/tmp"}),", we will use these later when starting the Hadoop container:"]}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"mkdir -p /tmp/shared\nmkdir -p /tmp/shared/hadoop_xml\n"})}),"\n",(0,d.jsx)(o.h3,{id:"configure-etchosts",children:"Configure /etc/hosts"}),"\n",(0,d.jsxs)(o.p,{children:["On the host machine, add the following entry to ",(0,d.jsx)(o.code,{children:"/etc/hosts"}),":"]}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{children:"127.0.0.1 druid-hadoop-demo\n"})}),"\n",(0,d.jsx)(o.h3,{id:"start-the-hadoop-container",children:"Start the Hadoop container"}),"\n",(0,d.jsxs)(o.p,{children:["Once the ",(0,d.jsx)(o.code,{children:"/tmp/shared"})," folder has been created and the ",(0,d.jsx)(o.code,{children:"etc/hosts"})," entry has been added, run the following command to start the Hadoop container."]}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"docker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 2049:2049 -p 2122:2122 -p 8020:8020 -p 8021:8021 -p 8030:8030 -p 8031:8031 -p 8032:8032 -p 8033:8033 -p 8040:8040 -p 8042:8042 -p 8088:8088 -p 8443:8443 -p 9000:9000 -p 10020:10020 -p 19888:19888 -p 34455:34455 -p 49707:49707 -p 50010:50010 -p 50020:50020 -p 50030:50030 -p 50060:50060 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 51111:51111 -v /tmp/shared:/shared druid-hadoop-demo:3.3.6 /etc/bootstrap.sh -bash\n"})}),"\n",(0,d.jsx)(o.p,{children:"Once the container is started, your terminal will attach to a bash shell running inside the container:"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"Starting sshd:                                             [  OK  ]\n18/07/26 17:27:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nStarting namenodes on [druid-hadoop-demo]\ndruid-hadoop-demo: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-druid-hadoop-demo.out\nlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-druid-hadoop-demo.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-druid-hadoop-demo.out\n18/07/26 17:27:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-druid-hadoop-demo.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-druid-hadoop-demo.out\nstarting historyserver, logging to /usr/local/hadoop/logs/mapred--historyserver-druid-hadoop-demo.out\nbash-4.1#\n"})}),"\n",(0,d.jsxs)(o.p,{children:["The ",(0,d.jsx)(o.code,{children:"Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"})," warning messages can be safely ignored."]}),"\n",(0,d.jsx)(o.h4,{id:"accessing-the-hadoop-container-shell",children:"Accessing the Hadoop container shell"}),"\n",(0,d.jsx)(o.p,{children:"To open another shell to the Hadoop container, run the following command:"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{children:"docker exec -it druid-hadoop-demo bash\n"})}),"\n",(0,d.jsx)(o.h3,{id:"copy-input-data-to-the-hadoop-container",children:"Copy input data to the Hadoop container"}),"\n",(0,d.jsxs)(o.p,{children:["From the ",(0,d.jsx)(o.code,{children:"apache-druid-32.0.0"})," package root on the host, copy the ",(0,d.jsx)(o.code,{children:"quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz"})," sample data to the shared folder:"]}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"cp quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz /tmp/shared/wikiticker-2015-09-12-sampled.json.gz\n"})}),"\n",(0,d.jsx)(o.h3,{id:"setup-hdfs-directories",children:"Setup HDFS directories"}),"\n",(0,d.jsx)(o.p,{children:"In the Hadoop container's shell, run the following commands to setup the HDFS directories needed by this tutorial and copy the input data to HDFS."}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"cd /usr/local/hadoop/bin\n./hdfs dfs -mkdir /druid\n./hdfs dfs -mkdir /druid/segments\n./hdfs dfs -mkdir /quickstart\n./hdfs dfs -mkdir /user\n./hdfs dfs -chmod 777 /druid\n./hdfs dfs -chmod 777 /druid/segments\n./hdfs dfs -chmod 777 /quickstart\n./hdfs dfs -chmod -R 777 /tmp\n./hdfs dfs -chmod -R 777 /user\n./hdfs dfs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz\n"})}),"\n",(0,d.jsx)(o.p,{children:"If you encounter namenode errors when running this command, the Hadoop container may not be finished initializing. When this occurs, wait a couple of minutes and retry the commands."}),"\n",(0,d.jsx)(o.h2,{id:"configure-druid-to-use-hadoop",children:"Configure Druid to use Hadoop"}),"\n",(0,d.jsx)(o.p,{children:"Some additional steps are needed to configure the Druid cluster for Hadoop batch indexing."}),"\n",(0,d.jsx)(o.h3,{id:"copy-hadoop-configuration-to-druid-classpath",children:"Copy Hadoop configuration to Druid classpath"}),"\n",(0,d.jsx)(o.p,{children:"From the Hadoop container's shell, run the following command to copy the Hadoop .xml configuration files to the shared folder:"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"cp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\n"})}),"\n",(0,d.jsx)(o.p,{children:"From the host machine, run the following, where {PATH_TO_DRUID} is replaced by the path to the Druid package."}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"mkdir -p {PATH_TO_DRUID}/conf/druid/single-server/micro-quickstart/_common/hadoop-xml\ncp /tmp/shared/hadoop_xml/*.xml {PATH_TO_DRUID}/conf/druid/single-server/micro-quickstart/_common/hadoop-xml/\n"})}),"\n",(0,d.jsx)(o.h3,{id:"update-druid-segment-and-log-storage",children:"Update Druid segment and log storage"}),"\n",(0,d.jsxs)(o.p,{children:["In your favorite text editor, open ",(0,d.jsx)(o.code,{children:"conf/druid/auto/_common/common.runtime.properties"}),", and make the following edits:"]}),"\n",(0,d.jsx)(o.h4,{id:"disable-local-deep-storage-and-enable-hdfs-deep-storage",children:"Disable local deep storage and enable HDFS deep storage"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{children:"#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n"})}),"\n",(0,d.jsx)(o.h4,{id:"disable-local-log-storage-and-enable-hdfs-log-storage",children:"Disable local log storage and enable HDFS log storage"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{children:"#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n\n"})}),"\n",(0,d.jsx)(o.h3,{id:"restart-druid-cluster",children:"Restart Druid cluster"}),"\n",(0,d.jsx)(o.p,{children:"Once the Hadoop .xml files have been copied to the Druid cluster and the segment/log storage configuration has been updated to use HDFS, the Druid cluster needs to be restarted for the new configurations to take effect."}),"\n",(0,d.jsxs)(o.p,{children:["If the cluster is still running, CTRL-C to terminate the ",(0,d.jsx)(o.code,{children:"bin/start-druid"})," script, and re-run it to bring the Druid services back up."]}),"\n",(0,d.jsx)(o.h2,{id:"load-batch-data",children:"Load batch data"}),"\n",(0,d.jsx)(o.p,{children:"We've included a sample of Wikipedia edits from September 12, 2015 to get you started."}),"\n",(0,d.jsxs)(o.p,{children:["To load this data into Druid, you can submit an ",(0,d.jsx)(o.em,{children:"ingestion task"})," pointing to the file. We've included\na task that loads the ",(0,d.jsx)(o.code,{children:"wikiticker-2015-09-12-sampled.json.gz"})," file included in the archive."]}),"\n",(0,d.jsxs)(o.p,{children:["Let's submit the ",(0,d.jsx)(o.code,{children:"wikipedia-index-hadoop3.json"})," task:"]}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{className:"language-bash",children:"bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop3.json --url http://localhost:8081\n"})}),"\n",(0,d.jsx)(o.h2,{id:"querying-your-data",children:"Querying your data"}),"\n",(0,d.jsxs)(o.p,{children:["After the data load is complete, please follow the ",(0,d.jsx)(o.a,{href:"/docs/latest/tutorials/tutorial-query",children:"query tutorial"})," to run some example queries on the newly loaded data."]}),"\n",(0,d.jsx)(o.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,d.jsxs)(o.p,{children:["This tutorial is only meant to be used together with the ",(0,d.jsx)(o.a,{href:"/docs/latest/tutorials/tutorial-query",children:"query tutorial"}),"."]}),"\n",(0,d.jsx)(o.p,{children:"If you wish to go through any of the other tutorials, you will need to:"}),"\n",(0,d.jsxs)(o.ul,{children:["\n",(0,d.jsxs)(o.li,{children:["Shut down the cluster and reset the cluster state by removing the contents of the ",(0,d.jsx)(o.code,{children:"var"})," directory under the druid package."]}),"\n",(0,d.jsxs)(o.li,{children:["Revert the deep storage and task storage config back to local types in ",(0,d.jsx)(o.code,{children:"conf/druid/auto/_common/common.runtime.properties"})]}),"\n",(0,d.jsx)(o.li,{children:"Restart the cluster"}),"\n"]}),"\n",(0,d.jsx)(o.p,{children:'This is necessary because the other ingestion tutorials will write to the same "wikipedia" datasource, and later tutorials expect the cluster to use local deep storage.'}),"\n",(0,d.jsx)(o.p,{children:"Example reverted config:"}),"\n",(0,d.jsx)(o.pre,{children:(0,d.jsx)(o.code,{children:"#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\ndruid.storage.type=local\ndruid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\n#druid.storage.type=hdfs\n#druid.storage.storageDirectory=/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\ndruid.indexer.logs.type=file\ndruid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=/druid/indexing-logs\n\n"})}),"\n",(0,d.jsx)(o.h2,{id:"further-reading",children:"Further reading"}),"\n",(0,d.jsxs)(o.p,{children:["For more information on loading batch data with Hadoop, please see ",(0,d.jsx)(o.a,{href:"/docs/latest/ingestion/hadoop",children:"the Hadoop batch ingestion documentation"}),"."]})]})}function h(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,d.jsx)(o,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}}}]);